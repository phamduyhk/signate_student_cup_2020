{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import python library\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "## Read file\n",
    "file_name = '<file_name>'\n",
    "## Read file using pandas\n",
    "df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's English stopwords as variable called 'stop' and don't find synonym of those words.\n",
    "stop = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizing sentence into token for finding synonym.\n",
    "def make_tokenizer(texts):\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(texts)\n",
    "    return t\n",
    "\n",
    "tokenizer = make_tokenizer(df['Message'])    ## Message is column name\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['Message'])\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences(X, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionary of word index\n",
    "index_word = {}\n",
    "for word in tokenizer.word_index.keys():\n",
    "    index_word[tokenizer.word_index[word]] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## word list\n",
    "words = [value for key, value in index_word.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to find synonym of words \n",
    "import spacy\n",
    "nlp = spacy.load('en', parser=False)\n",
    "def check_lemma(t,w) :\n",
    "    r = [d for d in t if (nlp(d.text)[0].lemma_ != nlp(w.text)[0].lemma_)]\n",
    "    return r\n",
    "\n",
    "def get_word_synonym(word):\n",
    "  filtered_words = [w for w in word.vocab if (not w.lower_ in stop) and w.is_lower == word.is_lower and w.prob >= -15] ## (not w.lower_ in stop) and\n",
    "  similarity = sorted(filtered_words, key=lambda w: word.similarity(w), reverse=True)\n",
    "  filtered_similarity = check_lemma(similarity[:30], word)\n",
    "  return filtered_similarity[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Synonym dictionary\n",
    "synonym_dict = {}\n",
    "\n",
    "for word in words:\n",
    "    if (not check_oos(word)) :\n",
    "        synonym_dict.update({word : tuple([w.lower_ for w in get_word_synonym(nlp.vocab[word])])})\n",
    "        #print(word, \" : \", [w.lower_ for w in get_word_synonym(nlp.vocab[word])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only consider filtered synonym\n",
    "import collections\n",
    "value_occurrences = collections.Counter(synonym_dict.values())\n",
    "\n",
    "filtered_synonym = {key: value for key, value in synonym_dict.items() if value_occurrences[value] == 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for augmenting data by replacing words with synonym using spaCy\n",
    "## This might not be best best method compared to data augmentation using language translation\n",
    "\n",
    "import re\n",
    "import random\n",
    "sr = random.SystemRandom()\n",
    "split_pattern = re.compile(r'\\s+')\n",
    "def data_augmentation(message, aug_range=1) :\n",
    "    augmented_messages = []\n",
    "    for j in range(0,aug_range) :\n",
    "        new_message = \"\"\n",
    "        for i in filter(None, split_pattern.split(message)) :\n",
    "            new_message = new_message + \" \" + sr.choice(filtered_synonym.get(i,[i]))\n",
    "        augmented_messages.append(new_message)\n",
    "    return augmented_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionary for intent count\n",
    "## Intent is column name\n",
    "intent_count = df.Intent.value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get max intent count to match other minority classes through data augmentation\n",
    "import operator\n",
    "max_intent_count = max(intent_count.items(), key=operator.itemgetter(1))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop to interate all messages\n",
    "import numpy as np\n",
    "import math\n",
    "import tqdm\n",
    "newdf = pd.DataFrame()\n",
    "for intent, count in intent_count.items() :\n",
    "    count_diff = max_intent_count - count    ## Difference to fill\n",
    "    multiplication_count = math.ceil((count_diff)/count)  ## Multiplying a minority classes for multiplication_count times\n",
    "    if (multiplication_count) :\n",
    "        old_message_df = pd.DataFrame()\n",
    "        new_message_df = pd.DataFrame()\n",
    "        for message in tqdm.tqdm(df[df[\"Intent\"] == intent][\"Message\"]) :\n",
    "            ## Extracting existing minority class batch\n",
    "            dummy1 = pd.DataFrame([message], columns=['Message'])\n",
    "            dummy1[\"Intent\"] = intent\n",
    "            old_message_df = old_message_df.append(dummy1)\n",
    "            \n",
    "            ## Creating new augmented batch from existing minority class\n",
    "            new_messages = data_augmentation(message, language, multiplication_count)\n",
    "            dummy2 = pd.DataFrame(new_messages, columns=['Message'])\n",
    "            dummy2[\"Intent\"] = intent\n",
    "            new_message_df = new_message_df.append(dummy2)\n",
    "        \n",
    "        ## Select random data points from augmented data\n",
    "        new_message_df=new_message_df.take(np.random.permutation(len(new_message_df))[:count_diff])\n",
    "        \n",
    "        ## Merge existing and augmented data points\n",
    "        newdf = newdf.append([old_message_df,new_message_df])\n",
    "    else :\n",
    "        newdf = newdf.append(df[df[\"Intent\"] == intent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print count of all new data points\n",
    "newdf.Intent.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save newdf back to file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
